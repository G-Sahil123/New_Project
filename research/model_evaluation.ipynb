{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff191c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20025947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\New_Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78f2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb537966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\New_Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    root_dir: Path\n",
    "    model_path: Path\n",
    "    data_path: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DocumindAI.constants import *\n",
    "from src.DocumindAI.utils.common import read_yaml, create_directories,save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        eval_config = EvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            model_path = config.model_path,\n",
    "            data_path = config.data_path,\n",
    "            mlflow_uri= config.mlflow_uri,\n",
    "            all_params= params\n",
    "        )\n",
    "        return eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LayoutLMv3ForSequenceClassification\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36216dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "        self.client = MlflowClient()\n",
    "        self.metrics = {}\n",
    "        self.model = None\n",
    "\n",
    "    def load_model_and_processor(self):\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            self.config.model_path\n",
    "        )\n",
    "\n",
    "        self.model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
    "            self.config.model_path\n",
    "        )\n",
    "\n",
    "        self.model.eval()    \n",
    "    def load_dataset(self):\n",
    "        self.dataset = load_from_disk(self.config.data_path)\n",
    "        self.eval_dataset = self.dataset[\"test\"]\n",
    "\n",
    "    def evaluation(self):\n",
    "        self.load_model_and_processor()\n",
    "        self.load_dataset()\n",
    "        preds, labels, confidence = [],[],[]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.eval_dataset:\n",
    "\n",
    "                inputs = {\n",
    "                    k:torch.tensor(v).unsqueeze(0) \n",
    "                    for k,v in batch.items() if k!=\"labels\"\n",
    "                }\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits,dim=-1)\n",
    "\n",
    "                predicted_id = probs.argmax(dim=-1).item()\n",
    "                cnf = probs.max().item()\n",
    "\n",
    "                preds.append(predicted_id)\n",
    "                confidence.append(cnf)\n",
    "                labels.append(batch[\"labels\"])\n",
    "\n",
    "        acc = accuracy_score(labels,preds)\n",
    "        f1 = f1_score(labels,preds,average=\"weighted\")\n",
    "\n",
    "        self.metrics = {\n",
    "            \"accuracy\":acc,\n",
    "            \"f1_score\":f1,\n",
    "            \"confidence_scores_list\":confidence\n",
    "        }\n",
    "\n",
    "        self.save_metrics(self.metrics)\n",
    "\n",
    "    def save_metrics(self,metrics):\n",
    "        scores = {\"f1_score\": metrics[\"f1_score\"], \"accuracy\": metrics[\"accuracy\"],\"mean_confidence\": float(np.mean(metrics[\"confidence_scores_list\"]))}\n",
    "        save_json(path=Path(self.config.root_dir)/\"metrics.json\", data=scores)\n",
    "\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "        self.experiment_name = \"DocuMind-LayoutLMv3\"\n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "        \n",
    "        with mlflow.start_run(run_name=\"layoutlmv3-training\") as run:\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metric(\"accuracy\",self.metrics[\"accuracy\"])\n",
    "            mlflow.log_metric(\"f1_score\",self.metrics[\"f1_score\"])\n",
    "            mlflow.log_metric(\"mean_confidence\",np.mean(self.metrics[\"confidence_scores_list\"]))\n",
    "            mlflow.transformers.log_model(\n",
    "                transformers_model=self.model,\n",
    "                artifact_path=\"model\",\n",
    "            )\n",
    "\n",
    "            return run.info.run_id\n",
    "\n",
    "    def register_model(self):\n",
    "\n",
    "        experiment = self.client.get_experiment_by_name(\"DocuMind-LayoutLMv3\")\n",
    "        runs = self.client.search_runs(\n",
    "            experiment_ids = [experiment.experiment_id],\n",
    "            order_by =  [\n",
    "                f\"metrics.accuracy DESC\"\n",
    "            ],\n",
    "            max_results=1\n",
    "        )\n",
    "\n",
    "        if not runs:\n",
    "            raise RuntimeError(\"No runs found to register\")\n",
    "\n",
    "        best_run = runs[0]\n",
    "        model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "\n",
    "        model_version = mlflow.register_model(\n",
    "            model_uri = model_uri,\n",
    "            name=\"Registered_Model\"\n",
    "        )\n",
    "\n",
    "        version = model_version.version\n",
    "\n",
    "        self.client.set_registered_model_alias(\n",
    "            name = \"Registered_Model\",\n",
    "            alias = \"champion\",\n",
    "            version = version\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad9581",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evaluation_config()\n",
    "    evaluation = ModelEvaluation(eval_config)\n",
    "    evaluation.evaluation()\n",
    "    evaluation.log_into_mlflow()\n",
    "    evaluation.register_model()\n",
    "\n",
    "except Exception as e:\n",
    "   raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

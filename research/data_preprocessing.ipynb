{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e7079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284097b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930c36e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\New_Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model: Path\n",
    "    max_length: int\n",
    "    training_ratio: float\n",
    "    batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d10917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DocumindAI.constants import *\n",
    "from src.DocumindAI.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_preprocessing_config(self) -> DataPreprocessingConfig:\n",
    "        config = self.config.data_preprocessing\n",
    "        params = self.params.preprocessing\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_preprocessing_config = DataPreprocessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model = config.model,\n",
    "            max_length = params.max_length,\n",
    "            training_ratio = params.training_ratio\n",
    "            batch_size = params.batch_size\n",
    "        )\n",
    "\n",
    "        return data_preprocessing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc16c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Value\n",
    "from transformers import AutoProcessor\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.preprocessor = AutoProcessor.from_pretrained(self.config.model,apply_ocr=True)\n",
    "\n",
    "        self.raw_dataset = {}\n",
    "        self.encoded_dataset = {}\n",
    "        self.label2id = {}\n",
    "        self.id2label = {}\n",
    "        self.num_labels = 0\n",
    "\n",
    "    def create_dataframe_for_split(self,split_name):\n",
    "        print(f\"Gathering file paths for the {split_name} split...\")\n",
    "        split_path = os.path.join(self.config.data_path, split_name)\n",
    "        data = []\n",
    "\n",
    "        for label_name in os.listdir(split_path):\n",
    "            class_dir = os.path.join(split_path, label_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    if filename.lower().endswith(('.tif', '.tiff', '.png', '.jpg', '.jpeg')):\n",
    "                        data.append({\n",
    "                            'image_path': os.path.join(class_dir, filename),\n",
    "                            'label': label_name,\n",
    "                        })\n",
    "\n",
    "        return data\n",
    "\n",
    "    def load_raw_dataset(self):\n",
    "        train_data = self.create_dataframe_for_split('train')\n",
    "        val_data = self.create_dataframe_for_split('val')\n",
    "        test_data = self.create_dataframe_for_split('test')\n",
    "\n",
    "        self.raw_dataset = {\n",
    "            'train': Dataset.from_list(train_data),\n",
    "            'val': Dataset.from_list(val_data),\n",
    "            'test': Dataset.from_list(test_data)\n",
    "        }\n",
    "\n",
    "        self.raw_dataset['train'] = self.raw_dataset['train'].shuffle(seed=42).select(range(1800))\n",
    "        self.raw_dataset['val'] = self.raw_dataset['val'].shuffle(seed=42).select(range(600))\n",
    "        self.raw_dataset['test'] = self.raw_dataset['test'].shuffle(seed=42).select(range(600))\n",
    "\n",
    "        print(\"\\n✅ Raw datasets loaded and shuffled.\")  \n",
    "\n",
    "    def encode_labels(self):\n",
    "        label_list = sorted(list(set(self.raw_dataset['test']['label'])))\n",
    "        self.label2id = {label: i for i, label in enumerate(label_list)}\n",
    "        self.num_labels = len(label_list)\n",
    "        self.id2label = {i:label for label,i in self.label2id.items()}\n",
    "        print(f\"\\nDetected Labels ({self.num_labels}): {label_list}\")\n",
    "\n",
    "        def map_label_to_id(examples):\n",
    "            examples['labels'] = [self.label2id[label] for label in examples['label']]\n",
    "            return examples\n",
    "\n",
    "        self.raw_dataset = {\n",
    "            split: self.raw_dataset[split].map(map_label_to_id, batched=True)\n",
    "            for split in self.raw_dataset\n",
    "        }\n",
    "        \n",
    "\n",
    "    def preprocess_data(self, examples):\n",
    "        images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\n",
    "        encoding = self.preprocessor(\n",
    "            images=images,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.config.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding['labels'] = torch.tensor(examples['labels'], dtype=torch.long)\n",
    "        return encoding\n",
    "\n",
    "    def apply_preprocessing(self):\n",
    "        print(\"\\nApplying LayoutLMv2 Processor (OCR, Fusion, and Tokenization)...\")\n",
    "        for split_name, ds in self.raw_dataset.items():\n",
    "            self.encoded_dataset[split_name] = ds.map(\n",
    "                self.preprocess_data,\n",
    "                batched=True,\n",
    "                batch_size=self.config.batch_size,\n",
    "                remove_columns=ds.column_names,\n",
    "                desc=f\"Preprocessing {split_name} Split\"\n",
    "            )\n",
    "            self.encoded_dataset[split_name].set_format(type=\"torch\")\n",
    "\n",
    "        print(\"\\n✅ Preprocessing complete.\")\n",
    "        print(f\"Number of classes: {self.num_labels}\")\n",
    "\n",
    "    def save_datasets(self, save_raw_path, save_encoded_path):\n",
    "        os.makedirs(save_raw_path, exist_ok=True)\n",
    "        os.makedirs(save_encoded_path, exist_ok=True)\n",
    "\n",
    "        print(\"\\nSaving encoded datasets...\")\n",
    "        for split_name, ds in self.encoded_dataset.items():\n",
    "            ds.save_to_disk(f\"{save_encoded_path}/{split_name}\")\n",
    "        print(\"✅ Encoded dataset saved successfully!\")\n",
    "\n",
    "        print(\"\\nSaving raw datasets...\")\n",
    "        for split_name, ds in self.raw_dataset.items():\n",
    "            ds.save_to_disk(f\"{save_raw_path}/{split_name}\")\n",
    "        print(\"✅ Raw dataset saved successfully!\")   \n",
    "\n",
    "    def preprocess(self):\n",
    "        self.load_raw_dataset()\n",
    "        self.encode_labels()\n",
    "        self.apply_preprocessing()\n",
    "    \n",
    "        base_dir = self.config.root_dir\n",
    "    \n",
    "        self.save_datasets(\n",
    "            save_raw_path=os.path.join(base_dir, \"raw_dataset\"),\n",
    "            save_encoded_path=os.path.join(base_dir, \"encoded_data\")\n",
    "        )\n",
    "\n",
    "        # Save preprocessor separately\n",
    "        preprocessor_dir = os.path.join(base_dir, \"preprocessor\")\n",
    "        os.makedirs(preprocessor_dir, exist_ok=True)\n",
    "        self.preprocessor.save_pretrained(preprocessor_dir)\n",
    "\n",
    "        print(f\"✅ Preprocessor saved at: {preprocessor_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_preprocessing_config = config.get_data_preprocessing_config()\n",
    "    data_preprocessing = DataPreprocessing(config=data_preprocessing_config)\n",
    "    data_preprocessing.preprocess()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

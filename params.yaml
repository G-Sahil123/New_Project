# config/params.yaml
preprocessing:
  max_length: 512
  training_ratio: 0.2
  batch_size: 32

TrainingArguments:
  num_labels: 6
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 1e-5
  weight_decay: 0.01
  eval_strategy: "epoch"
  save_strategy: "epoch"
  load_best_model_at_end: True
  remove_unused_columns: False
  optim: "adamw_torch"
  number_of_unfreeze_layers: 6

# model:
#   vision_encoder: "efficientnet_b3"
#   text_encoder: "bert-base-uncased" 
#   fusion_layers: [512, 256, 128]
#   dropout_rate: 0.3

# document_classes:
#   - "letter"
#   - "form"
#   - "email"
#   - "invoice"
#   - "news_article"
#   - "resume"